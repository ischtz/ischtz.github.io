[{"authors":["admin"],"categories":null,"content":"I am a researcher in experimental psychology at Justus-Liebig University Giessen, Germany. My research is focused on how we as humans sense, perceive, and act on our environment.\nEven during a simple every-day task, such as taking a sip of coffee while reading this sentence, our brain continuously interprets and reacts to the world in a highly dynamic sensorimotor loop. To unravel this loop, I use a variety of methods such as eye tracking, hand and body tracking, psychophysics, and computational modeling techniques. Most of my research makes use of virtual reality to create well-controlled yet highly naturalistic environments. On the applied side, I am interested in how sensorimotor principles can inform and improve our interactions with technology and lead to more human-centric computing. Finally, I also support other research projects through code, statistical analysis, and virtual experience design.\nI graduated with a degree in psychology from Philipps University Marburg. In 2015, I successfully defended my PhD (Dr. rer. nat) in experimental psychology in the Perception \u0026amp; Action lab of Prof. Dr. Katja Fiehler at Giessen University. My postdoctoral research experience since then includes helping establish the Physics of Cognition lab at Chemnitz University of Technology (Prof. Dr. Wolfgang Einhäuser-Treyer) and a two-year industry postdoctoral assignment at Facebook Reality Labs (Redmond WA, USA).\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a researcher in experimental psychology at Justus-Liebig University Giessen, Germany. My research is focused on how we as humans sense, perceive, and act on our environment.\nEven during a simple every-day task, such as taking a sip of coffee while reading this sentence, our brain continuously interprets and reacts to the world in a highly dynamic sensorimotor loop.","tags":null,"title":"Immo Schuetz","type":"authors"},{"authors":[],"categories":[],"content":"","date":1603191361,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603191361,"objectID":"7f5a602c72ea0d6019d949a68b1eeec8","permalink":"/project/gaze-interaction/","publishdate":"2020-10-20T12:56:01+02:00","relpermalink":"/project/gaze-interaction/","section":"project","summary":"","tags":[],"title":"Eye Gaze for Natural Interactions","type":"project"},{"authors":[],"categories":[],"content":"","date":1603191281,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603191281,"objectID":"0d36f8ab1b3b867f490913f697ec1bf5","permalink":"/project/natural-scene-fixations/","publishdate":"2020-10-20T12:54:41+02:00","relpermalink":"/project/natural-scene-fixations/","section":"project","summary":"","tags":[],"title":"Gaze Fixations in Natural Scenes","type":"project"},{"authors":["Immo Schuetz","T Scott Murdison","Marina Zannoli"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603187311,"objectID":"5edb9b0396e7e7e3ee4309d9df8fd2a7","permalink":"/publication/etra-2020-psychophysics/","publishdate":"2020-10-20T15:23:51.852711Z","relpermalink":"/publication/etra-2020-psychophysics/","section":"publication","summary":"Eye gaze promises to be a fast and intuitive way of interacting with technology. Importantly, the performance of a gaze selection paradigm depends on the eye tracker used: Higher tracking accuracy allows for selection of smaller targets, and higher precision and sampling rate allow for faster and more robust interaction. Here we present a novel approach to predict the minimal eye tracker specifications required for gaze-based selection. We quantified selection performance for targets of different sizes while recording high-fidelity gaze data. Selection performance across target sizes was well modeled by a sigmoid similar to a psychometric function. We then simulated lower tracker fidelity by adding noise, a constant spatial bias, or temporal sub-sampling of the recorded data while re-fitting the model each time. Our approach can inform design by predicting performance for a given interface element and tracker fidelity or the minimal element size for a specific performance level.","tags":[],"title":"A Psychophysics-inspired Model of Gaze Selection Performance","type":"publication"},{"authors":["Stephan Joachim Garbin","Oleg Komogortsev","Robert Cavin","Gregory Hughes","Yiru Shen","Immo Schuetz","Sachin S Talathi"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603207139,"objectID":"739b6ba1da56398c889c62344c4707a2","permalink":"/publication/etra-2020-dataset-eye-tracking/","publishdate":"2020-10-20T15:23:53.431719Z","relpermalink":"/publication/etra-2020-dataset-eye-tracking/","section":"publication","summary":"We present a large scale data set of eye-images captured using a virtual-reality (VR) head mounted display mounted with two synchronized eye-facing cameras at a frame rate of 200 Hz under controlled illumination. This dataset is compiled from video capture of the eye-region collected from 152 individual participants and is divided into four subsets: (i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil and sclera (ii) 252,690 unlabeled eye-images, (iii) 91,200 frames from randomly selected video sequences of 1.5 seconds in duration, and (iv) 143 pairs of left and right point cloud data compiled from corneal topography of eye regions collected from a subset, 143 out of 152, participants in the study. A baseline experiment has been evaluated on the dataset for the task of semantic segmentation of pupil, iris, sclera and background, with the mean intersection-over-union (mIoU) of 98.3 %. We anticipate that this dataset will create opportunities to researchers in the eye tracking community and the broader machine learning and computer vision community to advance the state of eye-tracking for VR applications, which in its turn will have greater implications in Human-Computer Interaction.","tags":["\"appearance-based eye tracking\"","\"eye tracking\"","\"sclera\"","\"iris\"","\"virtual reality\"","\"pupil\"","\"segmentation\""],"title":"Dataset for Eye Tracking on a Virtual Reality Platform","type":"publication"},{"authors":["Immo Schuetz","T Scott Murdison","Kevin J MacKenzie","Marina Zannoli"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603187310,"objectID":"f4fbaf274c642f51214925b3fe5f3d4c","permalink":"/publication/chi-2019-fitts-like-performance/","publishdate":"2020-10-20T15:23:51.711156Z","relpermalink":"/publication/chi-2019-fitts-like-performance/","section":"publication","summary":"Eye gaze as an input method has been studied since the 1990s, to varied results: some studies found gaze to be more efficient than traditional input methods like a mouse, others far behind. Comparisons are often backed up by Fitts' Law without explicitly acknowledging the ballistic nature of saccadic eye movements. Using a vision science-inspired model, we here show that a Fitts'-like distribution of movement times can arise due to the execution of secondary saccades, especially when targets are small. Study participants selected circular targets using gaze. Seven different target sizes and two saccade distances were used. We then determined performance across target sizes for different sampling windows (\\\"dwell times\\\") and predicted an optimal dwell time range. Best performance was achieved for large targets reachable by a single saccade. Our findings highlight that Fitts' Law, while a suitable approximation in some cases, is an incomplete description of gaze interaction dynamics.","tags":[],"title":"An Explanation of Fitts' Law-like Performance in Gaze-Based Selection Tasks Using a Psychophysics Approach","type":"publication"},{"authors":["Peter Veto","Immo Schütz","Wolfgang Einhäuser"],"categories":[],"content":"","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603207138,"objectID":"7ba6e72beb112560116a1aa45a694c9d","permalink":"/publication/jov-2018-cfp/","publishdate":"2020-10-20T15:23:52.727069Z","relpermalink":"/publication/jov-2018-cfp/","section":"publication","summary":" Diverse paradigms, including ambiguous stimuli and mental imagery, have suggested a shared representation between motor and perceptual domains. We examined the effects of manual action on ambiguous perception in a continuous flash suppression (CFS) experiment. Specifically, we asked participants to try to perceive a suppressed grating while rotating a manipulandum. In one condition, the grating's motion was fully controlled by the manipulandum movement; in another condition the coupling was weak; and in a third condition, no movement was executed. We found no effect of the movement condition on the subjectively reported visibility of the grating, which is in contrast to previous studies that allowed for more top-down influence. However, we did observe an effect on eye movements: the gain of the optokinetic nystagmus induced by the grating was modulated by its coupling to the manual movement. Our results (a) indicate that action-to-perception transfer can occur on different levels of perceptual organization, (b) demonstrate that CFS involves the shared representations between action and perception differently than paradigms used in earlier studies, and (c) highlight the importance of objective measures beyond subjective report when studying how action affects perception and awareness. ","tags":[],"title":"Continuous flash suppression: Manual action affects eye movements but not the reported percept","type":"publication"},{"authors":["Immo Schütz","Johanna Elisabeth Busch","Lukas Gorka","Wolfgang Einhäuser"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603207138,"objectID":"697b0f1ccaf34304234f27ca00f2495f","permalink":"/publication/joc-2018-pupil-fluctuations/","publishdate":"2020-10-20T15:23:52.974227Z","relpermalink":"/publication/joc-2018-pupil-fluctuations/","section":"publication","summary":"When a visual stimulus oscillates in luminance, pupil size follows this oscillation. Recently, it has been demonstrated that such induced pupil oscillations can be used to tag which stimulus is covertly attended. Here we ask whether this “pupil frequency tagging” approach can be extended to visual awareness, specifically to inferring perceptual dominance in Binocular Rivalry between complex stimuli. We presented two distinct stimuli, a face and a house, to each eye and modulated their luminance at 1.7 Hz either in counter-phase (180° phase shift), with a 90° phase shift or in phase (0° control). In some conditions, we additionally asked observers to attend either of the stimuli. The luminance modulation was sufficiently subtle that rivalry dynamics did not differ among these conditions, and was also indistinguishable from unmodulated presentation of the stimuli. For the 180° and the 90° phase-shifted stimuli, we found that the phase of the pupil response relative to the stimuli was modulated by perceptual dominance; that is, the relative phase depended on the stimulus the observer was aware of. In turn, this perceptually dominant stimulus could be decoded from the phase of the pupil response significantly above chance. Neither percept dependence of the phase nor significant decoding was found for the 0° control condition. Our results show that visual awareness modulates pupil responses and provide proof of principle that dominance in rivalry for complex stimuli can be inferred from induced pupil fluctuations.","tags":[],"title":"Visual Awareness in Binocular Rivalry Modulates Induced Pupil Fluctuations","type":"publication"},{"authors":["Antje Nuthmann","Wolfgang Einhäuser","Immo Schütz"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603204785,"objectID":"cd6516ce8a5ffba2a0eb09b5458174da","permalink":"/publication/fnhum-2017-glmm/","publishdate":"2020-10-20T15:23:51.997786Z","relpermalink":"/publication/fnhum-2017-glmm/","section":"publication","summary":"Since the turn of the millennium, a large number of computational models of visual salience have been put forward. How best to evaluate a given model's ability to predict where human observers fixate in images of real-world scenes remains an open research question. Assessing the role of spatial biases is a challenging issue; this is particularly true when we consider the tendency for high-salience items to appear in the image center, combined with a tendency to look straight ahead (“central bias”). This problem is further exacerbated in the context of model comparisons, because some—but not all—models implicitly or explicitly incorporate a center preference to improve performance. To address this and other issues, we propose to combine a-priori parcellation of scenes with generalized linear mixed models (GLMM), building upon previous work. With this method, we can explicitly model the central bias of fixation by including a central-bias predictor in the GLMM. A second predictor captures how well the saliency model predicts human fixations, above and beyond the central bias. By-subject and by-item random effects account for individual differences and differences across scene items, respectively. Moreover, we can directly assess whether a given saliency model performs significantly better than others. In this article, we describe the data processing steps required by our analysis approach. In addition, we demonstrate the GLMM analyses by evaluating the performance of different saliency models on a new eye-tracking corpus. To facilitate the application of our method, we make the open-source Python toolbox “GridFix” available.","tags":[],"title":"How well can saliency models predict fixation selection in scenes beyond central bias? A new approach to model evaluation using generalized linear mixed models","type":"publication"},{"authors":["Mathias Klinghammer","Immo Schütz","Gunnar Blohm","Katja Fiehler"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603207138,"objectID":"eef2fdc7a6d55d01cd3b58cb86f983d1","permalink":"/publication/visres-2016-reaching-depth/","publishdate":"2020-10-20T15:23:52.445596Z","relpermalink":"/publication/visres-2016-reaching-depth/","section":"publication","summary":"Previous research has demonstrated that humans use allocentric information when reaching to remembered visual targets, but most of the studies are limited to 2D space. Here, we study allocentric coding of memorized reach targets in 3D virtual reality. In particular, we investigated the use of allocentric information for memory-guided reaching in depth and the role of binocular and monocular (object size) depth cues for coding object locations in 3D space. To this end, we presented a scene with objects on a table which were located at different distances from the observer and served as reach targets or allocentric cues. After free visual exploration of this scene and a short delay the scene reappeared, but with one object missing (=reach target). In addition, the remaining objects were shifted horizontally or in depth. When objects were shifted in depth, we also independently manipulated object size by either magnifying or reducing their size. After the scene vanished, participants reached to the remembered target location on the blank table. Reaching endpoints deviated systematically in the direction of object shifts, similar to our previous results from 2D presentations. This deviation was stronger for object shifts in depth than in the horizontal plane and independent of observer-target-distance. Reaching endpoints systematically varied with changes in object size. Our results suggest that allocentric information is used for coding targets for memory-guided reaching in depth. Thereby, retinal disparity and vergence as well as object size provide important binocular and monocular depth cues.","tags":["\"Allocentric\"","\"Reference frames\"","\"Memory-guided reaching\"","\"Depth cues\"","\"Virtual reality\""],"title":"Allocentric information is used for memory-guided reaching in depth: A virtual reality study","type":"publication"},{"authors":["Katja Fiehler","Immo Schütz","Tina Meller","Lore Thaler"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603207432,"objectID":"b5c03169cd038616896dab407480b33c","permalink":"/publication/multsens-2015-echolocation/","publishdate":"2020-10-20T15:23:52.581907Z","relpermalink":"/publication/multsens-2015-echolocation/","section":"publication","summary":"Echolocation can be used by blind and sighted humans to navigate their environment. The current study investigated the neural activity underlying processing of path direction during walking. Brain activity was measured with fMRI in three blind echolocation experts, and three blind and three sighted novices. During scanning, participants listened to binaural recordings that had been made prior to scanning while echolocation experts had echolocated during walking along a corridor which could continue to the left, right, or straight ahead. Participants also listened to control sounds that contained ambient sounds and clicks, but no echoes. The task was to decide if the corridor in the recording continued to the left, right, or straight ahead, or if they were listening to a control sound. All participants successfully dissociated echo from no echo sounds, however, echolocation experts were superior at direction detection. We found brain activations associated with processing of path direction (contrast: echo vs. no echo) in superior parietal lobule (SPL) and inferior frontal cortex in each group. In sighted novices, additional activation occurred in the inferior parietal lobule (IPL) and middle and superior frontal areas. Within the framework of the dorso-dorsal and ventro-dorsal pathway proposed by Rizzolatti and Matelli (), our results suggest that blind participants may automatically assign directional meaning to the echoes, while sighted participants may apply more conscious, high-level spatial processes. High similarity of SPL and IFC activations across all three groups, in combination with previous research, also suggest that all participants recruited a multimodal spatial processing system for action (here: locomotion).","tags":[],"title":"Neural Correlates of Human Echolocation of Path Direction During Walking","type":"publication"},{"authors":["Immo Schütz","Denise YP Henriques","Katja Fiehler"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603207139,"objectID":"5a8e4f92917e9f3a3f5f4cfc9d98a25c","permalink":"/publication/ebr-2015-serial-reach-targets/","publishdate":"2020-10-20T15:23:53.229114Z","relpermalink":"/publication/ebr-2015-serial-reach-targets/","section":"publication","summary":"When reaching for remembered target locations, it has been argued that the brain primarily relies on egocentric metrics and especially target position relative to gaze when reaches are immediate, but that the visuo-motor system relies stronger on allocentric (i.e., object-centered) metrics when a reach is delayed. However, previous reports from our group have shown that reaches to single remembered targets are represented relative to gaze, even when static visual landmarks are available and reaches are delayed by up to 12 s. Based on previous findings which showed a stronger contribution of allocentric coding in serial reach planning, the present study aimed to determine whether delay influences the use of a gaze-dependent reference frame when reaching to two remembered targets in a sequence after a delay of 0, 5 or 12 s. Gaze was varied relative to the first and second target and shifted away from the target before each reach. We found that participants used egocentric and allocentric reference frames in combination with a stronger reliance on allocentric information regardless of whether reaches were executed immediately or after a delay. Our results suggest that the relative contributions of egocentric and allocentric reference frames for spatial coding and updating of sequential reach targets do not change with a memory delay between target presentation and reaching.","tags":[],"title":"No effect of delay on the spatial representation of serial reach targets","type":"publication"},{"authors":["Immo Schütz","Denise Y P Henriques","Katja Fiehler"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603204786,"objectID":"58e07967b4666ebc0b02d729d1410c18","permalink":"/publication/visres-2013-delayed-reaching/","publishdate":"2020-10-20T15:23:52.158926Z","relpermalink":"/publication/visres-2013-delayed-reaching/","section":"publication","summary":"Previous results suggest that the brain predominantly relies on a constantly updated gaze-centered target representation to guide reach movements when no other visual information is available. In the present study, we investigated whether the addition of reliable visual landmarks influences the use of spatial reference frames for immediate and delayed reaching. Subjects reached immediately or after a delay of 8 or 12s to remembered target locations, either with or without landmarks. After target presentation and before reaching they shifted gaze to one of five different fixation points and held their gaze at this location until the end of the reach. With landmarks present, gaze-dependent reaching errors were smaller and more precise than when reaching without landmarks. Delay influenced neither reaching errors nor variability. These findings suggest that when landmarks are available, the brain seems to still use gaze-dependent representations but combine them with gaze-independent allocentric information to guide immediate or delayed reach movements to visual targets.","tags":["\"Spatial updating\"","\"Spatial coding\"","\"Reaching\"","\"Landmarks\"","\"Delayed reaching\"","\"Reference frame\""],"title":"Gaze-centered spatial updating in delayed reaching even in the presence of landmarks","type":"publication"},{"authors":["Katja Fiehler","Immo Schütz","Denise Y P Henriques"],"categories":[],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603204786,"objectID":"39e1d653a07b5bbfc6b2f1acaba7c0e4","permalink":"/publication/visres-2011-memory-delays/","publishdate":"2020-10-20T15:23:52.303656Z","relpermalink":"/publication/visres-2011-memory-delays/","section":"publication","summary":"Previous research has demonstrated that remembered targets for reaching are coded and updated relative to gaze, at least when the reaching movement is made soon after the target has been extinguished. In this study, we want to test whether reach targets are updated relative to gaze following different time delays. Reaching endpoints systematically varied as a function of gaze relative to target irrespective of whether the action was executed immediately or after a delay of 5s, 8s or 12s. The present results suggest that memory traces for reach targets continue to be coded in a gaze-dependent reference frame if no external cues are present.","tags":["\"Movement\"","\"Reference frames\"","\"Remapping\"","\"Spatial memory\"","\"Spatial representation\""],"title":"Gaze-centered spatial updating of reach targets across different memory delays","type":"publication"}]